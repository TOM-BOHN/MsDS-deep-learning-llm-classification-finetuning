{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwlZkcWkuGMD"
      },
      "source": [
        "# Deep Learning: LLM: Classification Finetuning\n",
        "**Thomas Bohn**   --   **2025-09-30**\n",
        "\n",
        "{{xxxxx}}  \n",
        "\n",
        "--  [Main Report](https://github.com/TOM-BOHN/MsDS-deep-learning-llm-classification-finetuning/blob/main/deep-learning-llm-classification-finetuning.ipynb)  --  [Github Repo](https://github.com/TOM-BOHN/MsDS-deep-learning-llm-classification-finetuning)  --  [Presentation Slides](xxx)  --  [Presentation Video](xxx) --  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07lWDJH6vAuV"
      },
      "source": [
        "# 1.&nbsp;Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHhIXdBPvCDs"
      },
      "source": [
        "**Problem Statement**\n",
        "\n",
        "This project addresses the challenge of predicting which responses users will prefer in a head-to-head comparison between chatbots powered by large language models (LLMs). The core problem involves developing machine learning models that can accurately predict human preferences when comparing responses from different LLMs to the same prompts. This is fundamentally a classification problem where we must determine whether users will prefer model A's response, model B's response, or consider them tied, based on the prompt and both responses.\n",
        "\n",
        "**Why is it Important?**\n",
        "\n",
        "As large language models (LLMs) become increasingly integrated into our daily lives, ensuring their responses resonate with users is critical for successful human-AI interaction. The ability to predict user preferences has profound implications for developing more user-friendly AI systems, improving chatbot interactions, and advancing the field of reinforcement learning from human feedback (RLHF). This work directly contributes to creating AI systems that can better align with human values and preferences, ultimately leading to more widely accepted and effective AI-powered conversation systems.\n",
        "\n",
        "**Limitations of Existing Solutions**\n",
        "\n",
        "Current approaches to preference prediction face several significant limitations. Direct prompting of existing LLMs for preference predictions often suffers from systematic biases, including position bias (favoring responses presented first), verbosity bias (preferring overly verbose responses), and self-enhancement bias (models promoting their own responses). Additionally, traditional methods struggle with the nuanced understanding required to evaluate response quality across diverse conversation contexts and user preferences. These limitations highlight the need for specialized machine learning approaches that can learn from human preference data rather than relying on model introspection.\n",
        "\n",
        "**Contribution**\n",
        "\n",
        "This project contributes to the field by developing and evaluating multiple machine learning approaches for LLM preference prediction, including baseline supervised learning models, hyperparameter optimization, and advanced classification techniques. The work provides a comprehensive comparison of different methodologies on real-world preference data from the Chatbot Arena, offering insights into which approaches work best for this challenging classification task. The project also demonstrates practical implementation strategies for handling large-scale text classification problems with imbalanced datasets.\n",
        "\n",
        "**Related Kaggle Competition**\n",
        "\n",
        "[LLM Classification Finetuning](https://www.kaggle.com/competitions/llm-classification-finetuning/overview)\n",
        "\n",
        "**DataSet**\n",
        "\n",
        "The dataset consists of user interactions from the Chatbot Arena, containing approximately 55,000 training examples and 25,000 test examples. Each interaction includes a user prompt, responses from two different LLMs (model A and model B), and human judge preferences indicating which response was preferred or if they were considered tied. The dataset represents real-world human preferences across diverse conversation topics and response styles, providing a rich foundation for developing preference prediction models. The text data may contain profane, vulgar, or offensive content, reflecting the authentic nature of human-AI interactions in the wild.\n",
        "\n",
        "**Data Facts**\n",
        "\n",
        "- **Dataset Size**:\n",
        "  - ~55,000 training examples\n",
        "  - ~25,000 test examples\n",
        "  - 184.19 MB\n",
        "- **Features**: \n",
        "  - `id`: Unique identifier for each row\n",
        "  - `prompt`: User input given to both models\n",
        "  - `response_a`: Response from model A\n",
        "  - `response_b`: Response from model B\n",
        "  - `model_a`/`model_b`: Model identities (training only)\n",
        "  - `winner_model_a`/`winner_model_b`/`winner_tie`: Binary target columns\n",
        "- **Task**: Multi-class classification (3 classes: model A wins, model B wins, tie)\n",
        "- **Data Source**: Chatbot Arena user interactions with human judge preferences\n",
        "- **Content Warning**: Contains potentially profane, vulgar, or offensive text\n",
        "- **Evaluation Metric**: Log loss with \"eps=auto\" for probability predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview of Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "{{xxxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detect Environment\n",
        "\n",
        "Determine if the notebook is running in Colab or Kaggle. Then change how the notebook behaves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detect Environment\n",
        "import os\n",
        "gIS_COLAB = 'COLAB_GPU' in os.environ or 'COLAB_TPU' in os.environ or 'COLAB_CPU' in os.environ\n",
        "gIS_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
        "print(\"Is Kaggle?\", gIS_KAGGLE, \" | \", \"Is Colab?\", gIS_COLAB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add Colab Only Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install the necessary packages\n",
        "import os\n",
        "if gIS_COLAB:\n",
        "    # Install Colab Specific Tooling\n",
        "    from google.colab import userdata\n",
        "    from google.colab import files\n",
        "\n",
        "    # Mount the Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Install the necessary packages\n",
        "    !pip install -q tensorflow\n",
        "    !pip install -q kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add Kaggle Only Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install the necessary packages\n",
        "if gIS_KAGGLE:\n",
        "    from kaggle_datasets import KaggleDatasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9augt6L_Wnb"
      },
      "source": [
        "## Common Python Libraries\n",
        "\n",
        "The following python libraries are used in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "pjoVn_82_Rya",
        "outputId": "0c19792c-ae58-4e05-89bb-ac5327391e69"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: num2words in /usr/local/lib/python3.10/dist-packages (0.5.13)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from num2words) (0.6.2)\n",
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.10/dist-packages (0.8.1)\n"
          ]
        }
      ],
      "source": [
        "# File system manangement\n",
        "import time, datetime, psutil, os\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "# Data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import random\n",
        "\n",
        "# Install text storage and manipulation\n",
        "import re\n",
        "import json\n",
        "import pickle\n",
        "import textwrap\n",
        "from tqdm import tqdm\n",
        "\n",
        "##################################\n",
        "\n",
        "# Plotting and visualization\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "sns.set_theme()\n",
        "\n",
        "# Train-test split and cross validation\n",
        "from sklearn.model_selection import train_test_split, ParameterGrid\n",
        "\n",
        "# Model evaluation\n",
        "from sklearn import metrics\n",
        "# Model Evaluation\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import (\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    classification_report,\n",
        "    accuracy_score\n",
        ")\n",
        "\n",
        "# Import Tensor Flow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras_nlp\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, History\n",
        "\n",
        "##################################\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"\n",
        "\n",
        "##################################\n",
        "\n",
        "print(f'Keras: {keras.__version__}')\n",
        "print(f'KerasNLP: {keras_nlp.__version__}')\n",
        "print(f'Tensorflow: {tf.__version__}')\n",
        "\n",
        "##################################\n",
        "\n",
        "# Suppress SyntaxWarnings for invalid escape sequences\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=SyntaxWarning, message='invalid escape sequence')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Memory monitoring and cleanup functions\n",
        "def get_memory_usage():\n",
        "    \"\"\"Get current memory usage in MB\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_info = process.memory_info()\n",
        "    return memory_info.rss / 1024 / 1024  # Convert to MB\n",
        "\n",
        "def print_memory_usage(stage=\"\"):\n",
        "    \"\"\"Print current memory usage\"\"\"\n",
        "    memory_mb = get_memory_usage()\n",
        "    print(f\"Memory usage {stage}: {memory_mb:.1f} MB\")\n",
        "\n",
        "def cleanup_memory():\n",
        "    \"\"\"Clean up memory by clearing TensorFlow cache and running garbage collection\"\"\"\n",
        "    import gc\n",
        "    # Clear TensorFlow cache\n",
        "    tf.keras.backend.clear_session()\n",
        "    # Force garbage collection\n",
        "    gc.collect()\n",
        "    print(\"Memory cleaned up\")\n",
        "\n",
        "def clear_session_between_models():\n",
        "    \"\"\"Clear TensorFlow session between model experiments to avoid variable conflicts\"\"\"\n",
        "    import gc\n",
        "    # Clear TensorFlow session and cache\n",
        "    tf.keras.backend.clear_session()\n",
        "    # Force garbage collection\n",
        "    gc.collect()\n",
        "    print(\"Session cleared - ready for next model experiment\")\n",
        "\n",
        "# Print initial memory usage\n",
        "print_memory_usage(\"at startup\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connect to TPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TPU (Tensor Processing Unit) Setup for Accelerated Training\n",
        "# This code attempts to connect to Google's TPU infrastructure for faster model training\n",
        "# TPUs are specialized hardware designed specifically for machine learning workloads\n",
        "\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('âœ… TPU found:', tpu.master())\n",
        "except:\n",
        "    print(\"âŒ No TPU found. Falling back to CPU/GPU.\")\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    # Connect to the TPU cluster.\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    # Initialize the TPU system for use\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    # Create a TPU distribution strategy for multi-core TPU usage\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "else:\n",
        "    # Use the default strategy for CPU/GPU.\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "# Print the number of replicas (cores) available for parallel processing\n",
        "print('Number of replicas:', strategy.num_replicas_in_sync)\n",
        "\n",
        "# Set up automatic tuning for data pipeline performance optimization\n",
        "# AUTOTUNE allows TensorFlow to automatically determine the optimal number of parallel calls\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Print TensorFlow version for reference\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connect to GPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "print(tf.test.gpu_device_name())\n",
        "\n",
        "# Configure GPU memory growth (prevents OOM errors)\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y39B3sJjvTD1"
      },
      "source": [
        "## Global Variables\n",
        "\n",
        "The following are global variables referenced in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vfjyMW6K_SS-"
      },
      "outputs": [],
      "source": [
        "# Recording the starting time, complemented with a stopping time check in the end to compute process runtime\n",
        "start = time.time()\n",
        "\n",
        "# Class representing the OS process and having memory_info() method to compute process memory usage\n",
        "process = psutil.Process(os.getpid())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "uLiPgQzL-Oiu",
        "outputId": "1daa3a4c-d833-4341-a5f4-e588be3b7163"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Level of Detail for functions is set to: 2\n"
          ]
        }
      ],
      "source": [
        "# Global Debug flag used to turn on and off more chatty blocks of code\n",
        "gDEBUG = True\n",
        "print('Debug is set to:', gDEBUG)\n",
        "# Global Level of Detail of table stats and details\n",
        "gLOD = 2\n",
        "print('Level of Detail for functions is set to:', gLOD)\n",
        "\n",
        "# Use environment global variables\n",
        "gIS_COLAB = gIS_COLAB\n",
        "gIS_KAGGLE = gIS_KAGGLE\n",
        "print(\"Is Kaggle?\", gIS_KAGGLE, \" | \", \"Is Colab?\", gIS_COLAB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook Configuration\n",
        "\n",
        "The following class consolidates key model configuration into an early cell to allow for easy access and adjustment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CFG:\n",
        "    \"\"\"\n",
        "    Configuration class for LLM Classification Finetuning project.\n",
        "    Organizes parameters by model type: baseline, experiments 1-3, and final design.\n",
        "    Models use baseline parameters unless overridden by model-specific parameters.\n",
        "    \"\"\"\n",
        "    \n",
        "    # =============================================================================\n",
        "    # GLOBAL CONFIGURATION PARAMETERS\n",
        "    # =============================================================================\n",
        "    seed = 27  # Random seed for reproducibility\n",
        "    sample_frac = 0.1  # Fraction of data to sample for development (0.1 = 10%)\n",
        "    \n",
        "    # =============================================================================\n",
        "    # BASELINE MODEL CONFIGURATION (Model 0: Baseline)\n",
        "    # =============================================================================\n",
        "    # DeBERTa v3 Components - Default Architecture\n",
        "    preset = \"deberta_v3_extra_small_en\"  # Name of pretrained DeBERTa model\n",
        "    preprocessor = keras_nlp.models.DebertaV3Preprocessor  # Text preprocessor\n",
        "    backbone = keras_nlp.models.DebertaV3Backbone  # Model backbone architecture\n",
        "    optimizer = keras.optimizers.Adam(5e-6)  # Adam optimizer with very low learning rate\n",
        "    # These parameters are used by the baseline model and serve as defaults\n",
        "    sequence_length = 512  # Input sequence length (tokens)\n",
        "    epochs = 5  # Training epochs\n",
        "    batch_size = 8  # Batch size for training\n",
        "    scheduler = 'cosine'  # Learning rate scheduler type\n",
        "    \n",
        "    # =============================================================================\n",
        "    # EXPERIMENT 1 CONFIGURATION (Model 1: AdamW Optimizer)\n",
        "    # =============================================================================\n",
        "    # Uses baseline parameters with AdamW optimizer instead of Adam\n",
        "    preset_1 = \"deberta_v3_extra_small_en\"  # Name of pretrained DeBERTa model\n",
        "    preprocessor_1 = keras_nlp.models.DebertaV3Preprocessor  # Text preprocessor\n",
        "    backbone_1 = keras_nlp.models.DebertaV3Backbone  # Model backbone architecture\n",
        "    optimizer_1 = keras.optimizers.AdamW(5e-6, weight_decay=0.01)  # Adam optimizer with very low learning rate\n",
        "\n",
        "    \n",
        "    # =============================================================================\n",
        "    # EXPERIMENT 2 CONFIGURATION (Model 2: Larger Context)\n",
        "    # =============================================================================\n",
        "    # Uses a larger DeBERTa v3 model with extended context window (1K+ tokens)\n",
        "    preset_2 = \"deberta_v3_small_en\"  # Larger model variant for extended context\n",
        "    preprocessor_2 = keras_nlp.models.DebertaV3Preprocessor  # Text preprocessor\n",
        "    backbone_2 = keras_nlp.models.DebertaV3Backbone  # Model backbone architecture\n",
        "    optimizer_2 = keras.optimizers.Adam(5e-6)  # Adam optimizer with very low learning rate\n",
        "\n",
        "    sequence_length_2 = 1024  # Extended sequence length for larger context window\n",
        "    batch_size_2 = 4  # Reduced batch size due to increased memory requirements\n",
        "       \n",
        "    # =============================================================================\n",
        "    # EXPERIMENT 3 CONFIGURATION (Model 3: Enhanced Architecture)\n",
        "    # =============================================================================\n",
        "\n",
        "    preset_3 = \"deberta_v3_extra_small_en\"  # Name of pretrained DeBERTa model\n",
        "    preprocessor_3 = keras_nlp.models.DebertaV3Preprocessor  # Text preprocessor\n",
        "    backbone_3 = keras_nlp.models.DebertaV3Backbone  # Model backbone architecture\n",
        "    optimizer_3 = keras.optimizers.Adam(5e-6)  # Adam optimizer with very low learning rate\n",
        "\n",
        "    # Enhanced model architecture with multi-layer classification head\n",
        "    hidden_units_3 = [512, 256, 64]  # Hidden layer sizes\n",
        "    dropout_rates_3 = [0.3, 0.2, 0.1]  # Dropout rates for each layer\n",
        "    learning_rate_3 = 1e-5  # Slightly higher learning rate for enhanced model\n",
        "    batch_size_3 = 8  # Adjusted batch size for enhanced model\n",
        "    \n",
        "    # =============================================================================\n",
        "    # FINAL DESIGN CONFIGURATION (Model 4: Optimized Final Model)\n",
        "    # =============================================================================\n",
        "    # Extended training configuration for the final optimized model\n",
        "    final__epochs = 10  # Extended epochs for final design model\n",
        "    final_design_batch_size = 8  # Reduced batch size due to increased memory requirements\n",
        "    \n",
        "    # =============================================================================\n",
        "    # MODEL TRAINING CONTROL VARIABLES\n",
        "    # =============================================================================\n",
        "    # Set to True to enable training, False to skip training for each model\n",
        "    train_baseline = False       # Model 0: Baseline model\n",
        "    train_experiment_1 = False   # Model 1: AdamW Optimizer experiment\n",
        "    train_experiment_2 = True    # Model 2: Larger Context experiment\n",
        "    train_experiment_3 = True    # Model 3: Enhanced Model Architecture\n",
        "    train_final_design = True    # Model 4: Final design (may run extended epochs)\n",
        "    \n",
        "    # =============================================================================\n",
        "    # CLASSIFICATION LABELS CONFIGURATION\n",
        "    # =============================================================================\n",
        "    # Mapping between class labels and names for the 3-class classification problem\n",
        "    label2name = {0: 'winner_model_a', 1: 'winner_model_b', 2: 'winner_tie'}\n",
        "    name2label = {v: k for k, v in label2name.items()}  # Reverse mapping\n",
        "    class_labels = list(label2name.keys())  # [0, 1, 2]\n",
        "    class_names = list(label2name.values())  # ['winner_model_a', 'winner_model_b', 'winner_tie']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'label2name:   {CFG.label2name}')\n",
        "print(f'name2label:   {CFG.name2label}')\n",
        "print(f'class_labels: {CFG.class_labels}')\n",
        "print(f'class_names:  {CFG.class_names}')\n",
        "\n",
        "print(f'\\nTraining Control Variables:')\n",
        "print(f'train_baseline:      {CFG.train_baseline}')\n",
        "print(f'train_experiment_1:  {CFG.train_experiment_1}')\n",
        "print(f'train_experiment_2:  {CFG.train_experiment_2}')\n",
        "print(f'train_experiment_3:  {CFG.train_experiment_3}')\n",
        "print(f'train_final_design:  {CFG.train_final_design}')\n",
        "\n",
        "# Sets value for random seed to produce similar result in each run.\n",
        "keras.utils.set_random_seed(CFG.seed)\n",
        "print(f'Keras Random Seed: {CFG.seed}.')\n",
        "\n",
        "# Decrease Precision to reduce memory usage\n",
        "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "print(f'Mixed Precision: \"mixed_float16\".')\n",
        "\n",
        "# Additional memory optimizations\n",
        "# Limit GPU memory growth to prevent OOM errors\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Set memory growth to True to allocate memory as needed\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print('GPU memory growth enabled.')\n",
        "    except RuntimeError as e:\n",
        "        print(f'GPU memory growth setup failed: {e}')\n",
        "\n",
        "# Set memory limit for GPU (optional - uncomment if needed)\n",
        "# tf.config.experimental.set_memory_limit(gpus[0], 4096)  # Limit to 4GB\n",
        "\n",
        "# Ensure eager execution is enabled to fix numpy() conversion issues\n",
        "tf.config.run_functions_eagerly(True)\n",
        "print('Eager execution enabled for all functions.')\n",
        "\n",
        "# Enable gradient checkpointing to reduce memory usage\n",
        "# This trades computation for memory by recomputing gradients instead of storing them\n",
        "tf.config.optimizer.set_jit(True)  # Enable XLA compilation for better performance\n",
        "print('Gradient checkpointing and XLA compilation enabled for memory optimization.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnEP1IbB-1dF"
      },
      "source": [
        "# 2.&nbsp;Data Source\n",
        "\n",
        "In this section, the code loads the dataset from Google Drive.\n",
        "\n",
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E54ZVhqGH0o1"
      },
      "source": [
        "## Import the Data (Kaggle or Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "s47OY0wa-9VR",
        "outputId": "73c66d41-1e90-4c72-dd74-eecd25769ef1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#print('os.environ: ', os.environ)\n",
        "\n",
        "if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
        "    print(\"Detected Kaggle environment - using Kaggle datasets\")\n",
        "elif 'COLAB_GPU' in os.environ or 'COLAB_TPU' in os.environ or 'COLAB_CPU' in os.environ:\n",
        "    print(\"Detected Google Colab environment - using local datasets\")\n",
        "else:\n",
        "    print(\"YIKES! I don't know where I am !!!!!!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr1LwEFwAUTh"
      },
      "outputs": [],
      "source": [
        "# Environment Detection and Dataset Loading\n",
        "# Detect whether we're running in Kaggle or Google Colab and load datasets accordingly\n",
        "\n",
        "if gIS_KAGGLE:\n",
        "    print(\"Detected Kaggle environment - using Kaggle datasets\")\n",
        "\n",
        "    # Dataset Path Configuration for Kaggle Environment\n",
        "    # This allows access to the competition datasets stored in Kaggle's cloud storage\n",
        "    GCS_PATH = '/kaggle/input/llm-classification-finetuning'\n",
        "\n",
        "    # Load Dataset for train\n",
        "    train_path = os.path.join(GCS_PATH, 'train.csv')\n",
        "    train_df = pd.read_csv(train_path)\n",
        "    print('Train Dataset Size:', len(train_df))\n",
        "\n",
        "    # Load Dataset for test\n",
        "    test_path = os.path.join(GCS_PATH, 'test.csv')\n",
        "    test_df = pd.read_csv(test_path)\n",
        "    print('Test Dataset Size:', len(test_df))\n",
        "\n",
        "elif gIS_COLAB:\n",
        "    print(\"Detected Google Colab environment - using local datasets\")\n",
        "\n",
        "    # Define the source of the zipped data files\n",
        "    target_file = 'llm-classification-finetuning.zip'\n",
        "    source_path_root = '/content/drive/MyDrive/[1.4] MsDS Class Files/-- DTSA 5511 Deep Learning/data'\n",
        "    destination_path_root = '/content'\n",
        "\n",
        "    # Copy the files to the runtime\n",
        "    shutil.copy(source_path_root + '/' + target_file, destination_path_root + '/')\n",
        "\n",
        "    # Display the files in the destination directory\n",
        "    print('Files in destination directory:', os.listdir(destination_path_root + '/'))\n",
        "\n",
        "    # Unzip the files (this is slow)\n",
        "    zip_file_path = destination_path_root + '/' + target_file\n",
        "\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        # Extract all the contents into the specified folder\n",
        "        zip_ref.extractall(destination_path_root + '/' + 'llm-classification-finetuning')\n",
        "\n",
        "    print('Dataset extraction completed')\n",
        "\n",
        "    # Dataset Path Configuration for Google Colab Environment\n",
        "    # Set up local file paths for the extracted dataset files\n",
        "    COLAB_DATA_PATH = '/content/llm-classification-finetuning'\n",
        "\n",
        "    # Load Dataset for train\n",
        "    train_path = os.path.join(COLAB_DATA_PATH, 'train.csv')\n",
        "    train_df = pd.read_csv(train_path)\n",
        "    print('Train Dataset Size:', len(train_df))\n",
        "    \n",
        "    # Load Dataset for test\n",
        "    test_path = os.path.join(COLAB_DATA_PATH, 'test.csv')\n",
        "    test_df = pd.read_csv(test_path)\n",
        "    print('Test Dataset Size:', len(test_df))\n",
        "\n",
        "else:\n",
        "    print(\"YIKES! I don't know where I am !!!!!!\")\n",
        "\n",
        "# Verify the datasets are loaded correctly\n",
        "if len(train_df) > 0:\n",
        "    print(f\"Successfully loaded {len(train_df)} training records\")\n",
        "else:\n",
        "    print(\"No training files found. Check the dataset path.\")\n",
        "\n",
        "if len(test_df) > 0:\n",
        "    print(f\"Successfully loaded {len(test_df)} test records\")\n",
        "else:\n",
        "    print(\"No test files found. Check the dataset path.\")\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JAv-COuvtIa"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the first record (index 0)\n",
        "print(\"=\"*80)\n",
        "print(\"FIRST RECORD (Index 0) - ORIGINAL DATA:\")\n",
        "print(\"=\"*80)\n",
        "print(\"Original prompt:\", train_df['prompt'].iloc[0])\n",
        "print(\"\\nOriginal response_a:\", train_df['response_a'].iloc[0])\n",
        "print(\"\\nOriginal response_b:\", train_df['response_b'].iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_and_extract_text(raw_data_string):\n",
        "    try:\n",
        "        # and handle null values by replacing them with None\n",
        "        safe_dict = {'null': None}\n",
        "        \n",
        "        # Replace 'null' with None in the string before evaluation\n",
        "        safe_string = raw_data_string.replace('null', 'None')\n",
        "\n",
        "        # Use eval with a safer approach - only allow specific built-ins\n",
        "        evaluated_list = eval(safe_string, {\"__builtins__\": {}}, safe_dict)\n",
        "\n",
        "        cleaned_list = []\n",
        "        for raw_str in evaluated_list:\n",
        "            # Handle None values (which represent null in the original data)\n",
        "            if raw_str is None:\n",
        "                cleaned_list.append(\"\")\n",
        "                continue\n",
        "            \n",
        "            # Clean the string and handle escape sequences\n",
        "            cleaned_str = str(raw_str)\n",
        "            # Fix common escape sequence issues that cause SyntaxWarnings\n",
        "            cleaned_str = cleaned_str.replace('\\\\/', '/')  # Fix \\/ to /\n",
        "            cleaned_str = cleaned_str.replace('\\\\n', '\\n')  # Fix \\n to newline\n",
        "            cleaned_str = cleaned_str.replace('\\\\t', '\\t')  # Fix \\t to tab\n",
        "\n",
        "            # Encode and decode to handle potential malformed characters\n",
        "            encoded_bytes = cleaned_str.encode('utf-8', errors='ignore')\n",
        "            decoded_str = encoded_bytes.decode('utf-8', errors='ignore')\n",
        "            cleaned_list.append(decoded_str)\n",
        "\n",
        "        # Return the first item, or an empty string if the list is empty\n",
        "        return cleaned_list[0] if cleaned_list else \"\"\n",
        "\n",
        "    except:\n",
        "        # Fallback for any parsing errors - return the original string cleaned\n",
        "        try:\n",
        "            # If it's not a list representation, just clean the string directly\n",
        "            cleaned_str = str(raw_data_string)\n",
        "            # Fix common escape sequence issues\n",
        "            cleaned_str = cleaned_str.replace('\\\\/', '/')  # Fix \\/ to /\n",
        "            cleaned_str = cleaned_str.replace('\\\\n', '\\n')  # Fix \\n to newline\n",
        "            cleaned_str = cleaned_str.replace('\\\\t', '\\t')  # Fix \\t to tab\n",
        "            \n",
        "            # If it's not a list representation, just clean the string directly\n",
        "            encoded_bytes = cleaned_str.encode('utf-8', errors='ignore')\n",
        "            decoded_str = encoded_bytes.decode('utf-8', errors='ignore')\n",
        "            return decoded_str\n",
        "        except:\n",
        "            return \"\"\n",
        "\n",
        "def process_competition_data(input_df, label_map, sample_frac=None):\n",
        "\n",
        "    # Work on a copy to avoid modifying the original DataFrame in place\n",
        "    df = input_df.copy()\n",
        "\n",
        "    # Optionally sample the data to speed up development\n",
        "    if sample_frac:\n",
        "        df = df.sample(frac=sample_frac, random_state=1) # Using a random_state for consistent results\n",
        "\n",
        "    # List of all input text columns in the dataset to clean\n",
        "    input_cols = [\"prompt\", \"response_a\", \"response_b\"]\n",
        "\n",
        "    # Process each column with the same cleaning function\n",
        "    for col in input_cols:\n",
        "        if col in df.columns:\n",
        "            new_col_name = col + \"_clean\"\n",
        "            df[new_col_name] = df[col].map(clean_and_extract_text)\n",
        "\n",
        "    # List of all winner values columns in the dataset\n",
        "    winner_cols = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
        "\n",
        "    # Check if all required winner columns exist\n",
        "    if all(col in df.columns for col in winner_cols):\n",
        "        # Check if a label_map was provided, which is now necessary\n",
        "        if not label_map:\n",
        "            raise ValueError(\"Input DataFrame has winner columns, but no 'label_map' was provided.\")\n",
        "        # Create the new class label columns\n",
        "        df[\"class_name\"] = df[winner_cols].idxmax(axis=1)\n",
        "        df[\"class_label\"] = df['class_name'].map(label_map)\n",
        "    else:\n",
        "        print(\"Winner columns not found. Skipping label creation.\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process the training data\n",
        "train_p_df = process_competition_data(\n",
        "    input_df=train_df,\n",
        "    label_map=CFG.name2label,\n",
        "    sample_frac=CFG.sample_frac\n",
        ")\n",
        "\n",
        "# Show Sample\n",
        "print(\"Training data processed:\")\n",
        "print(f\"Shape: {train_p_df.shape}\")\n",
        "print(f\"Columns: {list(train_p_df.columns)}\")\n",
        "train_p_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process the testing data\n",
        "test_p_df = process_competition_data(\n",
        "    input_df=test_df,\n",
        "    label_map=CFG.name2label,\n",
        "    sample_frac=1\n",
        ")\n",
        "\n",
        "# Show Sample\n",
        "print(\"Testing data processed:\")\n",
        "print(f\"Shape: {test_p_df.shape}\")\n",
        "print(f\"Columns: {list(test_p_df.columns)}\")\n",
        "test_p_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preview Results of Data  Prep "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the first record (index 0)\n",
        "print(\"=\"*80)\n",
        "print(\"FIRST RECORD (Index 0) - ORIGINAL DATA:\")\n",
        "print(\"=\"*80)\n",
        "print(\"Original prompt:\", train_p_df['prompt'].iloc[0])\n",
        "print(\"\\nOriginal response_a:\", train_p_df['response_a'].iloc[0])\n",
        "print(\"\\nOriginal response_b:\", train_p_df['response_b'].iloc[0])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FIRST RECORD (Index 0) - CLEANED DATA:\")\n",
        "print(\"=\"*80)\n",
        "print(\"Cleaned prompt:\", train_p_df['prompt_clean'].iloc[0])\n",
        "print(\"\\nCleaned response_a:\", train_p_df['response_a_clean'].iloc[0])\n",
        "print(\"\\nCleaned response_b:\", train_p_df['response_b_clean'].iloc[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BtBwi2SKW2N"
      },
      "source": [
        "# 3.&nbsp;Exploratory Data Analysis (EDA)\n",
        "\n",
        "The EDA phase focuses on understanding the dataset, including data distribution and label counts. Various functions are used to inspect the structure of the dataset, visualize the label distribution, and assess the text length and word count of the documentation. The data is found to be somewhat imbalanced across categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcwHYK4SI7wY"
      },
      "source": [
        "## EDA Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_text_analysis_columns(df, text_column):\n",
        "    # Create a copy to avoid modifying the original dataframe\n",
        "    result_df = df.copy()\n",
        "    \n",
        "    # Add character length column\n",
        "    length_col = f\"{text_column}_length\"\n",
        "    result_df[length_col] = result_df[text_column].astype(str).str.len()\n",
        "    \n",
        "    # Add word count column\n",
        "    word_count_col = f\"{text_column}_word_count\"\n",
        "    result_df[word_count_col] = result_df[text_column].astype(str).str.split().str.len()\n",
        "    \n",
        "    # Add estimated tokens column (rough approximation: ~4 characters per token)\n",
        "    tokens_col = f\"{text_column}_estimated_tokens\"\n",
        "    result_df[tokens_col] = (result_df[length_col] / 4).round().astype(int)\n",
        "    \n",
        "    return result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview Sample Records in Train\n",
        "def preview_random_record(df, show_columns=None):\n",
        "    # Set random seed for reproducibility\n",
        "    random.seed(27)\n",
        "    \n",
        "    # Select a random index\n",
        "    random_idx = random.randint(0, len(df) - 1)\n",
        "    random_record = df.iloc[random_idx]\n",
        "    \n",
        "    # Default columns to show if not specified\n",
        "    if show_columns is None:\n",
        "        show_columns = ['id', 'prompt_clean', 'response_a_clean', 'response_b_clean', \n",
        "                       'model_a', 'model_b', 'class_name', 'class_label']\n",
        "    \n",
        "    print(f\"Random Record Preview (Index: {random_idx})\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Display each column with proper formatting\n",
        "    for col in show_columns:\n",
        "        if col in df.columns:\n",
        "            value = random_record[col]\n",
        "            \n",
        "            # Format text columns with word wrapping\n",
        "            if col in ['prompt_clean', 'response_a_clean', 'response_b_clean']:\n",
        "                print(f\"\\nðŸ”¹ {col.upper()}:\")\n",
        "                print(\"-\" * 40)\n",
        "                # Wrap text for better readability\n",
        "                wrapped_text = textwrap.fill(str(value), width=70, initial_indent=\"  \", subsequent_indent=\"  \")\n",
        "                print(wrapped_text)\n",
        "            else:\n",
        "                print(f\"\\nðŸ”¹ {col.upper()}: {value}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"Record Statistics:\")\n",
        "    print(f\"   - Prompt Word Count: {random_record['prompt_clean_word_count']} words\")\n",
        "    print(f\"   - Response A Word Count: {random_record['response_a_clean_word_count']} words\")\n",
        "    print(f\"   - Response B Word Count: {random_record['response_b_clean_word_count']} words\")\n",
        "    print(f\"   - Prompt Estimated Tokens: {random_record['prompt_clean_estimated_tokens']} words\")\n",
        "    print(f\"   - Response A Estimated Tokens: {random_record['response_a_clean_estimated_tokens']} words\")\n",
        "    print(f\"   - Response B Estimated Tokens: {random_record['response_b_clean_estimated_tokens']} words\")\n",
        "    return random_record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_llm_distribution(df, model_a_col='model_a', model_b_col='model_b', \n",
        "                         title='Distribution of LLMs', \n",
        "                         color_scale='viridis',\n",
        "                         x_axis_rotation=-45,\n",
        "                         fig_width=None, fig_height=None):\n",
        "\n",
        "    # Combine model columns and count occurrences\n",
        "    model_df = pd.concat([df[model_a_col], df[model_b_col]])\n",
        "    counts = model_df.value_counts().reset_index()\n",
        "    counts.columns = ['LLM', 'Count']\n",
        "    \n",
        "    # Create the bar plot\n",
        "    fig = px.bar(counts, x='LLM', y='Count',\n",
        "                 title=title,\n",
        "                 color='Count', \n",
        "                 color_continuous_scale=color_scale)\n",
        "    \n",
        "    # Update layout for better readability\n",
        "    fig.update_layout(\n",
        "        xaxis_tickangle=x_axis_rotation,\n",
        "        width=fig_width,\n",
        "        height=fig_height\n",
        "    )\n",
        "    \n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_winner_distribution(df, class_name_col='class_name', \n",
        "                           title='Winner Distribution', \n",
        "                           color_scale='viridis',\n",
        "                           fig_width=None, fig_height=None,\n",
        "                           show_values=True):\n",
        "\n",
        "    # Count occurrences of each winner class\n",
        "    counts = df[class_name_col].value_counts().reset_index()\n",
        "    counts.columns = ['Winner', 'Win Count']\n",
        "    \n",
        "    # Create the bar plot\n",
        "    fig = px.bar(counts, x='Winner', y='Win Count',\n",
        "                 title=title,\n",
        "                 labels={'Winner': 'Winner', 'Win Count': 'Win Count'},\n",
        "                 color='Winner', \n",
        "                 color_continuous_scale=color_scale)\n",
        "    \n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        xaxis_title=\"Winner\", \n",
        "        yaxis_title=\"Win Count\",\n",
        "        width=fig_width,\n",
        "        height=fig_height\n",
        "    )\n",
        "    \n",
        "    # Add count values on top of bars if requested\n",
        "    if show_values:\n",
        "        fig.update_traces(texttemplate='%{y}', textposition='outside')\n",
        "    \n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Common distribution function for word count + other stats visualizations\n",
        "def plot_word_count_distribution(df, column_name, title, fig_width=800, fig_height=500, \n",
        "                                bins=50, color='skyblue', show_stats=True):    \n",
        "    # Get the data\n",
        "    data = df[column_name].dropna()\n",
        "    \n",
        "    # Create the figure\n",
        "    fig, ax = plt.subplots(figsize=(fig_width/100, fig_height/100))\n",
        "    \n",
        "    # Create histogram\n",
        "    n, bins_edges, patches = ax.hist(data, bins=bins, color=color, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
        "    \n",
        "    # Add statistics if requested\n",
        "    if show_stats:\n",
        "        mean_val = data.mean()\n",
        "        median_val = data.median()\n",
        "        std_val = data.std()\n",
        "        \n",
        "        # Add vertical lines for mean and median\n",
        "        ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.1f}')\n",
        "        ax.axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Median: {median_val:.1f}')\n",
        "        \n",
        "        # Add text box with statistics\n",
        "        stats_text = f'Count: {len(data):,}\\nMean: {mean_val:.1f}\\nMedian: {median_val:.1f}\\nStd: {std_val:.1f}\\nMin: {data.min():.1f}\\nMax: {data.max():.1f}'\n",
        "        ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=10,\n",
        "                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "        \n",
        "        ax.legend()\n",
        "    \n",
        "    # Customize the plot\n",
        "    ax.set_xlabel('Word Count', fontsize=12)\n",
        "    ax.set_ylabel('Frequency', fontsize=12)\n",
        "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Format x-axis to show integer values\n",
        "    ax.ticklabel_format(style='plain', axis='x')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print summary statistics\n",
        "    print(f\"\\n{title} - Summary Statistics:\")\n",
        "    print(f\"Count: {len(data):,}\")\n",
        "    print(f\"Mean: {data.mean():.2f}\")\n",
        "    print(f\"Median: {data.median():.2f}\")\n",
        "    print(f\"Standard Deviation: {data.std():.2f}\")\n",
        "    print(f\"Minimum: {data.min():.0f}\")\n",
        "    print(f\"Maximum: {data.max():.0f}\")\n",
        "    print(f\"25th Percentile: {data.quantile(0.25):.2f}\")\n",
        "    print(f\"75th Percentile: {data.quantile(0.75):.2f}\")\n",
        "    \n",
        "    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bO3CvKfI_eC"
      },
      "source": [
        "## EDA Analysis: Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply the function to analyze the prompt column\n",
        "train_ps_df = add_text_analysis_columns(train_p_df, 'prompt_clean')\n",
        "train_ps_df = add_text_analysis_columns(train_ps_df, 'response_a_clean')\n",
        "train_ps_df = add_text_analysis_columns(train_ps_df, 'response_b_clean')\n",
        "\n",
        "test_ps_df = add_text_analysis_columns(test_p_df, 'prompt_clean')\n",
        "test_ps_df = add_text_analysis_columns(test_ps_df, 'response_a_clean')\n",
        "test_ps_df = add_text_analysis_columns(test_ps_df, 'response_b_clean')\n",
        "\n",
        "# Create total words column by summing prompt + response_a + response_b word counts\n",
        "train_ps_df['total_word_count'] = (\n",
        "    train_ps_df['prompt_clean_word_count'] + \n",
        "    train_ps_df['response_a_clean_word_count'] + \n",
        "    train_ps_df['response_b_clean_word_count']\n",
        ")\n",
        "\n",
        "# Create total token estimation column by summing prompt + response_a + response_b word counts\n",
        "train_ps_df['total_estimated_tokens'] = (\n",
        "    train_ps_df['prompt_clean_estimated_tokens'] + \n",
        "    train_ps_df['response_a_clean_estimated_tokens'] + \n",
        "    train_ps_df['response_b_clean_estimated_tokens']\n",
        ")\n",
        "\n",
        "# Create total words column by summing prompt + response_a + response_b word counts\n",
        "test_ps_df['total_word_count'] = (\n",
        "    test_ps_df['prompt_clean_word_count'] + \n",
        "    test_ps_df['response_a_clean_word_count'] + \n",
        "    test_ps_df['response_b_clean_word_count']\n",
        ")\n",
        "\n",
        "# Create total token estimation column by summing prompt + response_a + response_b word counts\n",
        "test_ps_df['total_estimated_tokens'] = (\n",
        "    test_ps_df['prompt_clean_estimated_tokens'] + \n",
        "    test_ps_df['response_a_clean_estimated_tokens'] + \n",
        "    test_ps_df['response_b_clean_estimated_tokens']\n",
        ")\n",
        "\n",
        "train_ps_stats = train_ps_df[[\n",
        "    'prompt_clean_estimated_tokens',\n",
        "    'response_a_clean_estimated_tokens',\n",
        "    'response_b_clean_estimated_tokens',\n",
        "    'total_estimated_tokens',\n",
        "    'prompt_clean_word_count',\n",
        "    'response_a_clean_word_count',\n",
        "    'response_b_clean_word_count',\n",
        "    'total_word_count'\n",
        "]].describe()\n",
        "\n",
        "train_ps_stats.round(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display a random record from the training data with word counts\n",
        "random_record = preview_random_record(train_ps_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display a random record from the training data with word counts\n",
        "random_record = preview_random_record(train_ps_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKqQq5X1yCDw"
      },
      "source": [
        "## EDA Analysis: Text Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LLM Dostribution in Training\n",
        "fig = plot_llm_distribution(\n",
        "    df = train_ps_df,\n",
        "    model_a_col='model_a',\n",
        "    model_b_col='model_b',\n",
        "    title='Training Data: LLM Distribution',\n",
        "    fig_width=800, fig_height=500\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Data Winner Distribution\n",
        "fig = plot_winner_distribution(\n",
        "    df = train_p_df, \n",
        "    title='Training Data: Winner Distribution',\n",
        "    color_scale='plasma',\n",
        "    fig_width=800, fig_height=500\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display distribution of estimated tokens of prompts\n",
        "plot_word_count_distribution(\n",
        "    df=train_ps_df, \n",
        "    column_name='prompt_clean_estimated_tokens',\n",
        "    title='Distribution of Estimated Tokens - Prompts',\n",
        "    fig_width=800, \n",
        "    fig_height=500,\n",
        "    color='lightblue'\n",
        ")\n",
        "\n",
        "# Display distribution of estimated tokens of response_a\n",
        "plot_word_count_distribution(\n",
        "    df=train_ps_df, \n",
        "    column_name='response_a_clean_estimated_tokens',\n",
        "    title='Distribution of Estimated Tokens - Response A',\n",
        "    fig_width=800, \n",
        "    fig_height=500,\n",
        "    color='lightblue'\n",
        ")\n",
        "\n",
        "# Display distribution of estimated tokens of response_b\n",
        "plot_word_count_distribution(\n",
        "    df=train_ps_df, \n",
        "    column_name='response_b_clean_estimated_tokens',\n",
        "    title='Distribution of Estimated Tokens - Response B',\n",
        "    fig_width=800, \n",
        "    fig_height=500,\n",
        "    color='lightblue'\n",
        ")\n",
        "\n",
        "# Display distribution of total estimated tokens\n",
        "plot_word_count_distribution(\n",
        "    df=train_ps_df, \n",
        "    column_name='total_estimated_tokens',\n",
        "    title='Distribution of Total Estimated Tokens (Prompt + Response A + Response B)',\n",
        "    fig_width=800, \n",
        "    fig_height=500,\n",
        "    color='lightsteelblue'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SnbCeUGydTo"
      },
      "source": [
        "## EDA Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpqY9vM_yefS"
      },
      "source": [
        "ADD HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_4RFPD2fkeh"
      },
      "source": [
        "# 4.&nbsp;Train-Validation-Test Split\n",
        "\n",
        "Split the dataset into training, validation, and test sets. Use tratified splitting to ensure that the class distribution remains consistent across these sets. The distribution of records across the labels is visualized to ensure a balanced split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Truncate Input Text to Fit Context Window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TO DO: Custom Truncation of Input Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Make Pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_pairs(row):\n",
        "    # Make options\n",
        "    row['options'] = [\n",
        "        f'Prompt: {row['prompt_clean']}\\n\\nResponse: {row['response_a_clean']}', # Response from Model A\n",
        "        f'Prompt: {row['prompt_clean']}\\n\\nResponse: {row['response_b_clean']}', # Response from Model B\n",
        "    ]\n",
        "    return row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_psp1_df = train_ps_df.apply(make_pairs, axis=1)\n",
        "test_psp1_df = test_ps_df.apply(make_pairs, axis=1)\n",
        "\n",
        "train_psp1_df['options'].iloc(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1ctkj7jNx4B"
      },
      "source": [
        "## Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the processed training data into training and validation sets\n",
        "split_train_psp1_df, split_valid_psp1_df = train_test_split(\n",
        "    train_psp1_df,                    # Source dataframe to split\n",
        "    test_size=0.2,                    # 20% of data goes to validation set\n",
        "    stratify=train_psp1_df['class_label']  # Maintain class distribution in both sets\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT6-MV_PN1CC"
      },
      "source": [
        "## Test Split Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shows the count of samples for each class label in the training data\n",
        "split_train_psp1_df['class_label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shows the count of samples for each class label in the training data\n",
        "split_valid_psp1_df['class_label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3xCg6WhgqXI"
      },
      "source": [
        "# 5.&nbsp;Preprocessing + Tokenization\n",
        "\n",
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap4Hw4S8Jd3E"
      },
      "source": [
        "## Core Normalization Functions - Default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the text preprocessor from configuration\n",
        "# This creates a tokenizer that will convert text to numerical tokens\n",
        "preprocessor = CFG.preprocessor.from_preset(\n",
        "    preset=CFG.preset,                    # Name of the model\n",
        "    sequence_length=CFG.sequence_length   # Max sequence length, will be padded if shorter\n",
        ")\n",
        "\n",
        "preprocessor\n",
        "\n",
        "# Function to preprocess individual text samples\n",
        "# Converts text to tokens and handles optional labels\n",
        "def preprocess_fn(text, label=None):\n",
        "    tokens = preprocessor(text)  # Tokenize the input text\n",
        "    return (tokens, label) if label is not None else (tokens, 0)\n",
        "\n",
        "# Function to build a TensorFlow dataset from text data\n",
        "# Handles preprocessing, batching, shuffling, and optimization\n",
        "def build_dataset(texts, labels=None, batch_size=32, cache=False, shuffle=1024):  # DISABLED cache for memory\n",
        "    AUTO = tf.data.AUTOTUNE  # Automatic optimization for parallel processing\n",
        "    \n",
        "    # Create dataset slices - handle both labeled and unlabeled data\n",
        "    slices = (texts, ) if labels is None \\\n",
        "        else (texts, keras.utils.to_categorical(labels, num_classes=3))\n",
        "    ds = tf.data.Dataset.from_tensor_slices(slices)\n",
        "    \n",
        "    # Cache dataset in memory for faster access during training\n",
        "    ds = ds.cache() if cache else ds\n",
        "    \n",
        "    # Apply preprocessing function to each sample in parallel\n",
        "    ds = ds.map(preprocess_fn, num_parallel_calls=AUTO)\n",
        "    \n",
        "    # Configure dataset options for shuffling\n",
        "    opt = tf.data.Options()\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(shuffle, seed=CFG.seed)  # Randomize order for better training\n",
        "        opt.experimental_deterministic = False   # Allow non-deterministic operations for performance\n",
        "    ds = ds.with_options(opt)\n",
        "    \n",
        "    # Create batches of specified size for training\n",
        "    ds = ds.batch(batch_size, drop_remainder=False)\n",
        "    \n",
        "    # Prefetch data to GPU/CPU for faster training\n",
        "    ds = ds.prefetch(AUTO)\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Normalization Functions - Large Context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the text preprocessor from configuration\n",
        "# This creates a tokenizer that will convert text to numerical tokens\n",
        "preprocessor_1000 = CFG.preprocessor_2.from_preset(\n",
        "    preset=CFG.preset_2,                    # Name of the model\n",
        "    sequence_length=CFG.sequence_length_2   # Max sequence length, will be padded if shorter\n",
        ")\n",
        "\n",
        "preprocessor_1000\n",
        "\n",
        "# Function to preprocess individual text samples\n",
        "# Converts text to tokens and handles optional labels\n",
        "def preprocess_fn_1000(text, label=None):\n",
        "    tokens = preprocessor_1000(text)  # Tokenize the input text\n",
        "    return (tokens, label) if label is not None else (tokens, 0)\n",
        "\n",
        "# Function to build a TensorFlow dataset from text data\n",
        "# Handles preprocessing, batching, shuffling, and optimization\n",
        "def build_dataset_1000(texts, labels=None, batch_size=32, cache=False, shuffle=1024):  # DISABLED cache for memory\n",
        "    AUTO = tf.data.AUTOTUNE  # Automatic optimization for parallel processing\n",
        "    \n",
        "    # Create dataset slices - handle both labeled and unlabeled data\n",
        "    slices = (texts, ) if labels is None \\\n",
        "        else (texts, keras.utils.to_categorical(labels, num_classes=3))\n",
        "    ds = tf.data.Dataset.from_tensor_slices(slices)\n",
        "    \n",
        "    # Cache dataset in memory for faster access during training\n",
        "    ds = ds.cache() if cache else ds\n",
        "    \n",
        "    # Apply preprocessing function to each sample in parallel\n",
        "    ds = ds.map(preprocess_fn_1000, num_parallel_calls=AUTO)\n",
        "    \n",
        "    # Configure dataset options for shuffling\n",
        "    opt = tf.data.Options()\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(shuffle, seed=CFG.seed)  # Randomize order for better training\n",
        "        opt.experimental_deterministic = False   # Allow non-deterministic operations for performance\n",
        "    ds = ds.with_options(opt)\n",
        "    \n",
        "    # Create batches of specified size for training\n",
        "    ds = ds.batch(batch_size, drop_remainder=False)\n",
        "    \n",
        "    # Prefetch data to GPU/CPU for faster training\n",
        "    ds = ds.prefetch(AUTO)\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gZaDQqdJoe_"
      },
      "source": [
        "## Build Training Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract text data from the training dataframe\n",
        "# Convert the 'options' column (containing processed text pairs) to a list\n",
        "train_texts = split_train_psp1_df.options.tolist()\n",
        "\n",
        "# Extract corresponding labels from the training dataframe\n",
        "# Convert the 'class_label' column (containing target classifications) to a list\n",
        "train_labels = split_train_psp1_df.class_label.tolist()\n",
        "\n",
        "# Build the training dataset using the custom build_dataset function\n",
        "# This creates an optimized TensorFlow dataset pipeline for training\n",
        "\n",
        "# Model 0 (Baseline) - Uses CFG.batch_size (8)\n",
        "train_ds = build_dataset(\n",
        "    train_texts,           \n",
        "    train_labels,          \n",
        "    batch_size=CFG.batch_size,  # 8\n",
        "    cache=True,\n",
        "    shuffle=True           \n",
        ")\n",
        "\n",
        "train_ds_1000 = build_dataset_1000(\n",
        "    train_texts,           \n",
        "    train_labels,          \n",
        "    batch_size=CFG.batch_size_2,  # 4\n",
        "    cache=True,\n",
        "    shuffle=True           \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Display the dataset object (shows dataset structure and configuration)\n",
        "train_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the dataset object (shows dataset structure and configuration)\n",
        "train_ds_1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build Validation Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract text data from the validation dataframe\n",
        "# Convert the 'options' column (containing processed text pairs) to a list\n",
        "valid_texts = split_valid_psp1_df.options.tolist()\n",
        "\n",
        "# Extract corresponding labels from the validation dataframe\n",
        "# Convert the 'class_label' column (containing target classifications) to a list\n",
        "valid_labels = split_valid_psp1_df.class_label.tolist()\n",
        "\n",
        "# Build the validation dataset using the custom build_dataset function\n",
        "# This creates an optimized TensorFlow dataset pipeline for validation\n",
        "\n",
        "# Model 0 (Baseline) - Uses CFG.batch_size (8)\n",
        "valid_ds = build_dataset(\n",
        "    valid_texts,\n",
        "    valid_labels,\n",
        "    batch_size=CFG.batch_size,  # 8\n",
        "    cache=True,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "valid_ds_1000 = build_dataset_1000(\n",
        "    valid_texts,\n",
        "    valid_labels,          \n",
        "    batch_size=CFG.batch_size_2,  # 8\n",
        "    cache=True,\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the dataset object (shows dataset structure and configuration)\n",
        "valid_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the dataset object (shows dataset structure and configuration)\n",
        "valid_ds_1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm-_DGFFmuSv"
      },
      "source": [
        "# 6.&nbsp;Define the Model Inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Results Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_training_metric(history, metric='loss', title=None):\n",
        "    \n",
        "    # Set default title if not provided\n",
        "    if title is None:\n",
        "        title = metric.replace('_', ' ').title()\n",
        "    \n",
        "    # Plot training and validation metrics\n",
        "    plt.plot(history.history[metric], label=f'training_{metric}')\n",
        "    plt.plot(history.history[f'val_{metric}'], label=f'validation_{metric}')\n",
        "    \n",
        "    # Customize plot\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(metric.replace('_', ' ').title())\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, class_names=None, figsize=(8, 6), title=\"Confusion Matrix\"):\n",
        "    \n",
        "    # Create confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    # Set default class names if not provided\n",
        "    if class_names is None:\n",
        "        class_names = [f'Class {i}' for i in range(len(cm))]\n",
        "    \n",
        "    # Create the plot\n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    \n",
        "    plt.title(title, fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Predicted Label', fontsize=12)\n",
        "    plt.ylabel('True Label', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print some statistics\n",
        "    print(f\"\\nConfusion Matrix Statistics:\")\n",
        "    print(f\"Total samples: {cm.sum()}\")\n",
        "    print(f\"Correct predictions: {cm.trace()}\")\n",
        "    print(f\"Accuracy: {cm.trace() / cm.sum():.4f}\")\n",
        "    \n",
        "    return cm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_metrics(pred_tag, y_test):\n",
        "    print(\"F1-score: \", f1_score(pred_tag, y_test, average='macro'))\n",
        "    print(\"Precision: \", precision_score(pred_tag, y_test, average='macro'))\n",
        "    print(\"Recall: \", recall_score(pred_tag, y_test, average='macro'))\n",
        "    print(\"Accuracy: \", accuracy_score(pred_tag, y_test))\n",
        "    print(\"-\"*50)\n",
        "    print(classification_report(pred_tag, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, valid_ds, history, model_name=\"Model\", show_plots=True):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EVALUATING {model_name.upper()}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Generate predictions\n",
        "    pred_val = model.predict(valid_ds, verbose=1)\n",
        "    print(\"number of predictions:\", len(pred_val))    \n",
        "    # Get predicted class labels (argmax of probabilities)\n",
        "    label_pred_val = np.argmax(pred_val, axis=1)\n",
        "    \n",
        "    # Show detailed metrics\n",
        "    show_metrics(label_pred_val, valid_labels)\n",
        "    \n",
        "    # Plot confusion matrix\n",
        "    print(f\"\\n{model_name} Confusion Matrix:\")\n",
        "    cm = plot_confusion_matrix(\n",
        "        y_true=valid_labels, \n",
        "        y_pred=label_pred_val,\n",
        "        class_names=CFG.class_names,\n",
        "        title=f\"{model_name} - Confusion Matrix\"\n",
        "    )\n",
        "    \n",
        "    # Plot training curves if requested\n",
        "    if show_plots:\n",
        "        print(f\"\\n{model_name} Training Curves:\")\n",
        "        # Plot loss curves\n",
        "        plot_training_metric(history, 'loss')\n",
        "        # Plot log loss curves\n",
        "        plot_training_metric(history, 'log_loss')\n",
        "        # Plot accuracy curves  \n",
        "        plot_training_metric(history, 'accuracy')\n",
        "    \n",
        "    # Return evaluation results\n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'f1_score': f1_score(label_pred_val, valid_labels, average='macro'),\n",
        "        'precision': precision_score(label_pred_val, valid_labels, average='macro'),\n",
        "        'recall': recall_score(label_pred_val, valid_labels, average='macro'),\n",
        "        'accuracy': accuracy_score(label_pred_val, valid_labels),\n",
        "        'confusion_matrix': cm\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWadgSEmJy8W"
      },
      "source": [
        "## Learning Rate Schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom learning rate scheduler function with cosine annealing\n",
        "# Creates a learning rate schedule that ramps up, sustains, then decays\n",
        "def get_lr_callback(batch_size=8, epochs=10, plot=False):\n",
        "    # Define learning rate parameters\n",
        "    lr_start, lr_max, lr_min = 1e-6, 0.3e-6 * batch_size, 1e-6  # Start, max, and minimum learning rates\n",
        "    lr_ramp_ep, lr_sus_ep, lr_decay = 2, 0, 0.8  # Ramp epochs, sustain epochs, decay factor\n",
        "\n",
        "    # Inner function that calculates learning rate for each epoch\n",
        "    def lrfn(epoch):\n",
        "        if epoch < lr_ramp_ep:\n",
        "            # Linear ramp-up phase: gradually increase from start to max\n",
        "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
        "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
        "            # Sustained phase: maintain maximum learning rate\n",
        "            lr = lr_max\n",
        "        else:\n",
        "            # Cosine decay phase: smooth decrease from max to min using cosine annealing\n",
        "            decay_total_epochs = epochs - (lr_ramp_ep + lr_sus_ep) - 1\n",
        "            decay_epoch_index = epoch - (lr_ramp_ep + lr_sus_ep)\n",
        "            phase = math.pi * decay_epoch_index / decay_total_epochs\n",
        "            # print(decay_total_epochs, decay_epoch_index, phase)  # Debug output\n",
        "            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n",
        "        return lr\n",
        "\n",
        "    # Optional visualization of the learning rate schedule\n",
        "    if plot:\n",
        "        plt.figure()\n",
        "        x_values = np.arange(1, epochs + 1)  # Create array of epoch numbers\n",
        "        lr_values = [lrfn(epoch) for epoch in range(epochs)]  # Calculate LR for each epoch\n",
        "        plt.plot(x_values, lr_values, marker='o')  # Plot LR curve\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('lr')\n",
        "        plt.ylim(0, 0.6e-5)  # Set y-axis limits for better visualization\n",
        "        plt.title('LR Scheduler')\n",
        "        plt.show()\n",
        "\n",
        "    # Return a Keras callback that applies the learning rate schedule during training\n",
        "    return keras.callbacks.LearningRateScheduler(lrfn, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build Model Measuement Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the loss function for multi-class classification\n",
        "# CategoricalCrossentropy is used when targets are one-hot encoded (3 classes: model A wins, model B wins, tie)\n",
        "log_loss = keras.metrics.CategoricalCrossentropy(name=\"log_loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whSsaEBx0a_L"
      },
      "source": [
        "## Model Checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to create ModelCheckpoint callbacks with customizable parameters\n",
        "# This allows for easy experimentation with different checkpointing strategies\n",
        "def create_model_checkpoint_callback(\n",
        "    filepath='best_model.weights.h5',  # File path where to save the best model weights\n",
        "    monitor='val_log_loss',            # Metric to monitor (default: validation loss)\n",
        "    save_best_only=True,               # Only save when the monitored metric improves\n",
        "    save_weights_only=True,            # Save only the model weights (not the full model)\n",
        "    mode='min',                        # Save when metric improves (min/max)\n",
        "    save_freq='epoch',                 # Save frequency ('epoch' or integer)\n",
        "    verbose=1,                         # Verbosity level (0=silent, 1=print messages)\n",
        "    name_suffix=\"\"                     # Optional suffix for file identification\n",
        "):\n",
        "    # Add suffix to filename if provided\n",
        "    if name_suffix:\n",
        "        base_name, ext = os.path.splitext(filepath)\n",
        "        filepath = f\"{base_name}{name_suffix}{ext}\"\n",
        "    \n",
        "    return keras.callbacks.ModelCheckpoint(\n",
        "        filepath=filepath,\n",
        "        monitor=monitor,\n",
        "        save_best_only=save_best_only,\n",
        "        save_weights_only=save_weights_only,\n",
        "        mode=mode,\n",
        "        save_freq=save_freq,\n",
        "        verbose=verbose\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to create EarlyStopping callbacks with customizable parameters\n",
        "# This allows for easy experimentation with different early stopping strategies\n",
        "def create_early_stopping_callback(\n",
        "    monitor='val_log_loss',      # Metric to monitor (default: validation loss)\n",
        "    min_delta=0.0001,           # Minimum change to qualify as an improvement\n",
        "    patience=3,                 # Number of epochs to wait before stopping\n",
        "    verbose=1,                  # Print messages when stopping is triggered\n",
        "    mode='min',                 # Stop when metric stops improving (min/max)\n",
        "    baseline=None,              # Baseline threshold for stopping\n",
        "    restore_best_weights=True,  # Restore best weights when stopping\n",
        "    start_from_epoch=0,         # Start monitoring from this epoch\n",
        "    name_suffix=\"\"              # Optional suffix for callback identification\n",
        "):\n",
        "\n",
        "    return keras.callbacks.EarlyStopping(\n",
        "        monitor=monitor,\n",
        "        min_delta=min_delta,\n",
        "        patience=patience,\n",
        "        verbose=verbose,\n",
        "        mode=mode,\n",
        "        baseline=baseline,\n",
        "        restore_best_weights=restore_best_weights,\n",
        "        start_from_epoch=start_from_epoch\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Epoch Time Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a time element to add to history of model training\n",
        "class EpochTimeHistory(Callback):\n",
        "    \"\"\"A custom Keras callback to record the duration of each epoch.\"\"\"\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        # Record the start time at the beginning of each epoch\n",
        "        self.epoch_start_time = time.time()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Calculate the epoch's duration\n",
        "        epoch_duration = time.time() - self.epoch_start_time\n",
        "\n",
        "        # Add the duration to the logs dictionary.\n",
        "        # Keras will automatically include this in the final history object.\n",
        "        if logs is not None:\n",
        "            logs['epoch_duration'] = epoch_duration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrJc1ZCi0IVf"
      },
      "source": [
        "# 7.&nbsp; Baseline Models: LLM Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO9FkdrHJ5US"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_model_baseline():\n",
        "    # Define input layers for the model\n",
        "    # The model expects two inputs: tokenized text and padding masks\n",
        "    # Shape (2, None) represents 2 sequences (prompt+response_a, prompt+response_b) of variable length\n",
        "    inputs = {\n",
        "        \"token_ids\": keras.Input(shape=(2, None), dtype=tf.int32, name=\"token_ids\"),      # Tokenized text input\n",
        "        \"padding_mask\": keras.Input(shape=(2, None), dtype=tf.int32, name=\"padding_mask\"), # Mask for padding tokens\n",
        "    }\n",
        "\n",
        "    # Create a DeBERTa v3 backbone model for text encoding\n",
        "    # This is the pre-trained transformer that will encode the text into embeddings\n",
        "    # Use the preset model configuration (deberta_v3_extra_small_en)\n",
        "    backbone = CFG.backbone.from_preset(CFG.preset)\n",
        "\n",
        "    # Process the first response pair: (Prompt + Response_A)\n",
        "    # Extract the first sequence from the input tensors (index 0)\n",
        "    response_a = {k: v[:, 0, :] for k, v in inputs.items()}\n",
        "    embed_a = backbone(response_a)  # Get embeddings for prompt + response A\n",
        "\n",
        "    # Process the second response pair: (Prompt + Response_B)\n",
        "    # Extract the second sequence from the input tensors (index 1)\n",
        "    response_b = {k: v[:, 1, :] for k, v in inputs.items()}\n",
        "    embed_b = backbone(response_b)  # Get embeddings for prompt + response B\n",
        "\n",
        "    # Combine the embeddings from both responses\n",
        "    # Concatenate embeddings along the last dimension to create a combined representation\n",
        "    embeds = keras.layers.Concatenate(axis=-1)([embed_a, embed_b])\n",
        "\n",
        "    # Apply global average pooling to reduce sequence dimension\n",
        "    # This creates a fixed-size representation regardless of input length\n",
        "    embeds = keras.layers.GlobalAveragePooling1D()(embeds)\n",
        "\n",
        "    # Final classification layer\n",
        "    # Dense layer with 3 outputs (model A wins, model B wins, tie) and softmax activation\n",
        "    outputs = keras.layers.Dense(3, activation=\"softmax\", name=\"classifier\")(embeds)\n",
        "\n",
        "    # Create the complete model by connecting inputs to outputs\n",
        "    model = keras.Model(inputs, outputs, name=\"Model_0_Baseline\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compile the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate the Model\n",
        "model_0 = create_model_baseline()\n",
        "# Define the log loss function for the model\n",
        "log_loss_0 = keras.metrics.CategoricalCrossentropy(name=\"log_loss\")\n",
        "# Create a fresh optimizer instance for this model to avoid variable conflicts\n",
        "optimizer_0 = keras.optimizers.Adam(5e-6)\n",
        "# Compile the model with optimizer, loss function, and evaluation metrics\n",
        "model_0.compile(\n",
        "    optimizer=optimizer_0,  # Fresh Adam optimizer with very low learning rate\n",
        "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.02),  # Loss with label smoothing\n",
        "    metrics=[\n",
        "        log_loss_0,  # Custom log loss metric for competition evaluation\n",
        "        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),  # Standard accuracy metric\n",
        "    ],\n",
        "    jit_compile=True # Disable XLA to fix eager execution issues\n",
        ")\n",
        "# Display the model architecture\n",
        "model_0.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eMVteAb01Vs"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if baseline model training is enabled\n",
        "if CFG.train_baseline:\n",
        "    print(\"Training Baseline Model (Model 0)...\")\n",
        "    print_memory_usage(\"before training\")\n",
        "    \n",
        "    # Create the learning rate callback with current configuration\n",
        "    # Uses batch size and epochs from CFG, and displays the LR schedule plot\n",
        "    lr_cb_0 = get_lr_callback(CFG.batch_size, CFG.epochs, plot=True)\n",
        "    # Create the default model checkpoint callback for current experiment\n",
        "    ckpt_cb_0 = create_model_checkpoint_callback(\n",
        "        filepath='model_0.best_weights.weights.h5'\n",
        "    )\n",
        "    # Create the default early stopping callback for current experiment\n",
        "    early_stopping_cb_0 = create_early_stopping_callback()\n",
        "    # Add time logging to the results\n",
        "    time_history_cb_0 = EpochTimeHistory()\n",
        "\n",
        "    # Start training the model\n",
        "    history_0 = model_0.fit(\n",
        "        train_ds,\n",
        "        epochs=CFG.epochs,\n",
        "        validation_data=valid_ds,\n",
        "        verbose = 1,\n",
        "        callbacks=[\n",
        "            lr_cb_0,\n",
        "            ckpt_cb_0,\n",
        "            early_stopping_cb_0,\n",
        "            time_history_cb_0\n",
        "        ]\n",
        "    )\n",
        "else:\n",
        "    print(\"Baseline Model training is DISABLED. Skipping training...\")\n",
        "    history_0 = None\n",
        "\n",
        "print_memory_usage(\"after training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process Results of Model (only if training was enabled):\n",
        "if CFG.train_baseline and history_0 is not None:\n",
        "    results_0 = evaluate_model(\n",
        "        model=model_0,\n",
        "        valid_ds=valid_ds, \n",
        "        history=history_0,\n",
        "        model_name=\"Baseline Model\",\n",
        "        show_plots=True\n",
        "    )\n",
        "else:\n",
        "    print(\"Baseline Model evaluation skipped - training was disabled.\")\n",
        "    results_0 = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_model_and_history(model, history, model_name, save_dir=None):\n",
        "    \n",
        "    # Set default save directory for Colab environment\n",
        "    if save_dir is None:\n",
        "        if gIS_COLAB:\n",
        "            save_dir = '/content/drive/MyDrive/[1.4] MsDS Class Files/-- DTSA 5511 Deep Learning/saved_models'\n",
        "        else:\n",
        "            save_dir = './saved_models'\n",
        "    \n",
        "    # Create save directory if it doesn't exist\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    \n",
        "    # Define file paths\n",
        "    model_path = os.path.join(save_dir, f'{model_name}.keras')\n",
        "    history_path = os.path.join(save_dir, f'{model_name}_history.pkl')\n",
        "    \n",
        "    # Save the model\n",
        "    model.save(model_path)\n",
        "    \n",
        "    # Save training history as pickle file (preserves exact data types)\n",
        "    with open(history_path, 'wb') as f:\n",
        "        pickle.dump(history.history, f)\n",
        "    \n",
        "    # Print confirmation\n",
        "    print(f\"Training history saved to:\")\n",
        "    print(f\"  - Model file: {model_path}\")\n",
        "    print(f\"  - History file: {history_path}\")\n",
        "    \n",
        "    return {\n",
        "        'model_path': model_path,\n",
        "        'history_path': history_path,\n",
        "        'model_name': model_name\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_0_paths = save_model_and_history(model_0, history_0, 'llm_0_baseline_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model_and_history(model_name, save_dir=None):\n",
        "\n",
        "    # Set default save directory for Colab environment\n",
        "    if save_dir is None:\n",
        "        if gIS_COLAB:\n",
        "            save_dir = '/content/drive/MyDrive/[1.4] MsDS Class Files/-- DTSA 5511 Deep Learning/saved_models'\n",
        "        else:\n",
        "            save_dir = './saved_models'\n",
        "    \n",
        "    # Define file paths\n",
        "    model_path = os.path.join(save_dir, f'{model_name}.keras')\n",
        "    history_path = os.path.join(save_dir, f'{model_name}_history.pkl')\n",
        "    \n",
        "    # Check if files exist\n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
        "    if not os.path.exists(history_path):\n",
        "        raise FileNotFoundError(f\"History file not found: {history_path}\")\n",
        "    \n",
        "    # Load the model\n",
        "    # Note: If you have custom objects, uncomment and modify the custom_objects line\n",
        "    # custom_objects = {'YourCustomClass': YourCustomClass}\n",
        "    # loaded_model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)\n",
        "    loaded_model = tf.keras.models.load_model(model_path)\n",
        "    \n",
        "    # Load the training history\n",
        "    with open(history_path, 'rb') as f:\n",
        "        loaded_history_dict = pickle.load(f)\n",
        "    \n",
        "    # Create a History object for compatibility\n",
        "    loaded_history = History()\n",
        "    loaded_history.history = loaded_history_dict\n",
        "    \n",
        "    # Set epoch information if available\n",
        "    if 'loss' in loaded_history_dict:\n",
        "        loaded_history.epoch = list(range(len(loaded_history_dict['loss'])))\n",
        "    else:\n",
        "        # Fallback: use the length of the first available metric\n",
        "        first_key = list(loaded_history_dict.keys())[0]\n",
        "        loaded_history.epoch = list(range(len(loaded_history_dict[first_key])))\n",
        "    \n",
        "    print(\"Model and history loaded successfully!\")\n",
        "    print(f\"Model path: {model_path}\")\n",
        "    print(f\"History path: {history_path}\")\n",
        "    print(f\"Model name: {model_name}\")\n",
        "    \n",
        "    return {\n",
        "        'model': loaded_model,\n",
        "        'history': loaded_history,\n",
        "        'model_path': model_path,\n",
        "        'history_path': history_path,\n",
        "        'model_name': model_name\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load baseline model\n",
        "baseline_0 = load_model_and_history('llm_0_baseline_model')\n",
        "model_0_loaded = baseline_0['model']\n",
        "history_0_loaded = baseline_0['history']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def review_training_history(history, model_name=\"Model\", show_epoch_times=True):\n",
        "    \n",
        "    # Handle both History objects and dictionaries\n",
        "    if hasattr(history, 'history'):\n",
        "        history_dict = history.history\n",
        "    else:\n",
        "        history_dict = history\n",
        "    \n",
        "    # Create DataFrame from history\n",
        "    history_df = pd.DataFrame(history_dict)\n",
        "    history_df.index += 1  # Start epoch numbering from 1\n",
        "    \n",
        "    # Round epoch duration to 2 decimal places if it exists\n",
        "    if 'epoch_duration' in history_df.columns:\n",
        "        history_df['epoch_duration'] = history_df['epoch_duration'].round(2)\n",
        "\n",
        "    # Display the results\n",
        "    print(f\"\\n--- {model_name} Training History ---\")\n",
        "    if show_epoch_times and 'epoch_duration' in history_df.columns:\n",
        "        print(\"(Includes epoch duration information)\")\n",
        "    \n",
        "    print(history_df)\n",
        "    \n",
        "    # Optional: Show summary statistics\n",
        "    print(f\"\\n--- {model_name} Summary Statistics ---\")\n",
        "    print(f\"Total epochs: {len(history_df)}\")\n",
        "    \n",
        "    return history_df\n",
        "\n",
        "# Example usage:\n",
        "# history_df = review_training_history(loaded_history_best, \"Best Model\")\n",
        "# history_df = review_training_history(history_0, \"Baseline Model\")\n",
        "# history_df = review_training_history(history_1, \"Experiment 1 Model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Review baseline model history\n",
        "baseline_history_df = review_training_history(history_0, \"llm_0_baseline_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process Results of Model (only if training was enabled):\n",
        "if CFG.train_baseline and history_0_loaded is not None:\n",
        "    results_0_loaded = evaluate_model(\n",
        "        model=model_0_loaded,\n",
        "        valid_ds=valid_ds, \n",
        "        history=history_0_loaded,\n",
        "        model_name=\"Baseline Model\",\n",
        "        show_plots=True\n",
        "    )\n",
        "else:\n",
        "    print(\"Baseline Model evaluation skipped - training was disabled.\")\n",
        "    results_0_loaded = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0vP9xMCnepL"
      },
      "source": [
        "# 8.1.&nbsp; Experiment 1: Train with AdamW Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pYPbH5L1bjZ"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The same model architecture is used for this experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtxev-Ul1pX9"
      },
      "source": [
        "## Compile the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile the model with optimizer, loss function, and evaluation metrics\n",
        "model_1 = create_model_baseline()\n",
        "# Define the log loss function for the model\n",
        "log_loss_1 = keras.metrics.CategoricalCrossentropy(name=\"log_loss\")\n",
        "# Create a fresh optimizer instance for this model to avoid variable conflicts\n",
        "optimizer_1 = keras.optimizers.AdamW(5e-6, weight_decay=0.01)\n",
        "# Compile the model with optimizer, loss function, and evaluation metrics\n",
        "model_1.compile(\n",
        "    optimizer=optimizer_1,  # Fresh AdamW optimizer with very low learning rate\n",
        "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.02),  # Loss with label smoothing\n",
        "    metrics=[\n",
        "        log_loss_1,  # Custom log loss metric for competition evaluation\n",
        "        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),  # Standard accuracy metric\n",
        "    ],\n",
        "    jit_compile=True # Disable XLA to fix eager execution issues\n",
        ")\n",
        "# Display the model architecture\n",
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if Experiment 1 model training is enabled\n",
        "if CFG.train_experiment_1:\n",
        "    print(\"Training Experiment 1 Model (Model 1 - AdamW Optimizer)...\")\n",
        "    print_memory_usage(\"before training\")\n",
        "    \n",
        "    # Create the learning rate callback with current configuration\n",
        "    # Uses batch size and epochs from CFG, and displays the LR schedule plot\n",
        "    lr_cb_1 = get_lr_callback(CFG.batch_size, CFG.epochs, plot=True)\n",
        "    # Create the default model checkpoint callback for current experiment\n",
        "    ckpt_cb_1 = create_model_checkpoint_callback(\n",
        "        filepath='model_1.best_weights.weights.h5'\n",
        "    )\n",
        "    # Create the default early stopping callback for current experiment\n",
        "    early_stopping_cb_1 = create_early_stopping_callback()\n",
        "    # Add time logging to the results\n",
        "    time_history_cb_1 = EpochTimeHistory()\n",
        "\n",
        "    # Start training the model\n",
        "    history_1 = model_1.fit(\n",
        "        train_ds,\n",
        "        epochs=CFG.epochs,\n",
        "        validation_data=valid_ds,\n",
        "        verbose = 1,\n",
        "        callbacks=[\n",
        "            lr_cb_1,\n",
        "            ckpt_cb_1,\n",
        "            early_stopping_cb_1,\n",
        "            time_history_cb_1\n",
        "        ]\n",
        "    )\n",
        "else:\n",
        "    print(\"Experiment 1 Model training is DISABLED. Skipping training...\")\n",
        "    history_1 = None\n",
        "\n",
        "print_memory_usage(\"after training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process Results of Model (only if training was enabled):\n",
        "if CFG.train_experiment_1 and history_1 is not None:\n",
        "    results_1 = evaluate_model(\n",
        "        model=model_1,\n",
        "        valid_ds=valid_ds, \n",
        "        history=history_1,\n",
        "        model_name=\"Experiment 1 Model (AdamW)\",\n",
        "        show_plots=True\n",
        "    )\n",
        "else:\n",
        "    print(\"Experiment 1 Model evaluation skipped - training was disabled.\")\n",
        "    results_1 = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8.2.&nbsp; Experiment 2: Larger Context Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_model_1000():\n",
        "\n",
        "    # Define input layers for the model\n",
        "    # The model expects two inputs: tokenized text and padding masks\n",
        "    # Shape (2, None) represents 2 sequences (prompt+response_a, prompt+response_b) of variable length\n",
        "    inputs = {\n",
        "        \"token_ids\": keras.Input(shape=(2, None), dtype=tf.int32, name=\"token_ids\"),      # Tokenized text input\n",
        "        \"padding_mask\": keras.Input(shape=(2, None), dtype=tf.int32, name=\"padding_mask\"), # Mask for padding tokens\n",
        "    }\n",
        "\n",
        "    # Create a DeBERTa v3 backbone model for text encoding with larger context\n",
        "    # Use the Experiment 2 preset model configuration (deberta_v3_small_en)\n",
        "    backbone = CFG.backbone_2.from_preset(CFG.preset_2)\n",
        "\n",
        "    # Process the first response pair: (Prompt + Response_A)\n",
        "    # Extract the first sequence from the input tensors (index 0)\n",
        "    response_a = {k: v[:, 0, :] for k, v in inputs.items()}\n",
        "    embed_a = backbone(response_a)  # Get embeddings for prompt + response A\n",
        "\n",
        "    # Process the second response pair: (Prompt + Response_B)\n",
        "    # Extract the second sequence from the input tensors (index 1)\n",
        "    response_b = {k: v[:, 1, :] for k, v in inputs.items()}\n",
        "    embed_b = backbone(response_b)  # Get embeddings for prompt + response B\n",
        "\n",
        "    # Combine the embeddings from both responses\n",
        "    # Concatenate embeddings along the last dimension to create a combined representation\n",
        "    embeds = keras.layers.Concatenate(axis=-1)([embed_a, embed_b])\n",
        "\n",
        "    # Apply global average pooling to reduce sequence dimension\n",
        "    # This creates a fixed-size representation regardless of input length\n",
        "    embeds = keras.layers.GlobalAveragePooling1D()(embeds)\n",
        "\n",
        "    # Final classification layer\n",
        "    # Dense layer with 3 outputs (model A wins, model B wins, tie) and softmax activation\n",
        "    outputs = keras.layers.Dense(3, activation=\"softmax\", name=\"classifier\")(embeds)\n",
        "\n",
        "    # Create the complete model by connecting inputs to outputs\n",
        "    model = keras.Model(inputs, outputs, name=\"Model_2_LargerContext\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compile the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile the model with optimizer, loss function, and evaluation metrics\n",
        "model_2 = create_model_1000()\n",
        "# Define the log loss function for the model\n",
        "log_loss_2 = keras.metrics.CategoricalCrossentropy(name=\"log_loss\")\n",
        "# Create a fresh optimizer instance for this model to avoid variable conflicts\n",
        "optimizer_2 = keras.optimizers.Adam(5e-6)\n",
        "# Compile the model with optimizer, loss function, and evaluation metrics\n",
        "model_2.compile(\n",
        "    optimizer=optimizer_2,  # Fresh Adam optimizer with very low learning rate\n",
        "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.02),  # Loss with label smoothing\n",
        "    metrics=[\n",
        "        log_loss_2,  # Custom log loss metric for competition evaluation\n",
        "        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),  # Standard accuracy metric\n",
        "    ],\n",
        "    jit_compile=True # Disable XLA to fix eager execution issues\n",
        ")\n",
        "# Display the model architecture\n",
        "model_2.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if Experiment 2 model training is enabled\n",
        "if CFG.train_experiment_2:\n",
        "    print(\"Training Experiment 2 Model (Model 2 - Larger Context Architecture)...\")\n",
        "    print_memory_usage(\"before training\")\n",
        "    \n",
        "    # Create the learning rate callback with current configuration\n",
        "    # Uses batch size and epochs from CFG, and displays the LR schedule plot\n",
        "    lr_cb_2 = get_lr_callback(CFG.batch_size_2, CFG.epochs, plot=True)\n",
        "    # Create the default model checkpoint callback for current experiment\n",
        "    ckpt_cb_2 = create_model_checkpoint_callback(\n",
        "        filepath='model_2.best_weights.weights.h5'\n",
        "    )\n",
        "    # Create the default early stopping callback for current experiment\n",
        "    early_stopping_cb_2 = create_early_stopping_callback()\n",
        "    # Add time logging to the results\n",
        "    time_history_cb_2 = EpochTimeHistory()\n",
        "\n",
        "    # Start training the model\n",
        "    history_2 = model_2.fit(\n",
        "        train_ds_1000,\n",
        "        epochs=CFG.epochs,\n",
        "        validation_data=valid_ds_1000,\n",
        "        verbose = 1,\n",
        "        callbacks=[\n",
        "            lr_cb_2,\n",
        "            ckpt_cb_2,\n",
        "            early_stopping_cb_2,\n",
        "            time_history_cb_2\n",
        "        ]\n",
        "    )\n",
        "else:\n",
        "    print(\"Experiment 2 Model training is DISABLED. Skipping training...\")\n",
        "    history_2 = None\n",
        "\n",
        "print_memory_usage(\"after training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process Results of Model (only if training was enabled):\n",
        "if CFG.train_experiment_2 and history_2 is not None:\n",
        "    results_2 = evaluate_model(\n",
        "        model=model_2,\n",
        "        valid_ds=valid_ds_1000, \n",
        "        history=history_2,\n",
        "        model_name=\"Experiment 2 Model (Model 2 - Larger Context)\",\n",
        "        show_plots=True\n",
        "    )\n",
        "else:\n",
        "    print(\"Experiment 2 Model evaluation skipped - training was disabled.\")\n",
        "    results_2 = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8.3.&nbsp; Experiment 3: Changed Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_model_enhanced():\n",
        "    # Define input layers for the model\n",
        "    # The model expects two inputs: tokenized text and padding masks\n",
        "    inputs = {\n",
        "        \"token_ids\": keras.Input(shape=(2, None), dtype=tf.int32, name=\"token_ids\"),      # Tokenized text input\n",
        "        \"padding_mask\": keras.Input(shape=(2, None), dtype=tf.int32, name=\"padding_mask\"), # Mask for padding tokens\n",
        "    }\n",
        "\n",
        "    # Create a DeBERTa v3 backbone model for text encoding\n",
        "    # This is the pre-trained transformer that will encode the text into embeddings\n",
        "    backbone = CFG.backbone_3.from_preset(CFG.preset_3)\n",
        "\n",
        "    # Process the first response pair: (Prompt + Response_A)\n",
        "    response_a = {k: v[:, 0, :] for k, v in inputs.items()}\n",
        "    embed_a = backbone(response_a)  # Get embeddings for prompt + response A\n",
        "\n",
        "    # Process the second response pair: (Prompt + Response_B)\n",
        "    response_b = {k: v[:, 1, :] for k, v in inputs.items()}\n",
        "    embed_b = backbone(response_b)  # Get embeddings for prompt + response B\n",
        "\n",
        "    # Concatenate embeddings along the last dimension to create a combined representation\n",
        "    embeds = keras.layers.Concatenate(axis=-1)([embed_a, embed_b])\n",
        "\n",
        "    # Apply global average pooling to reduce sequence dimension\n",
        "    embeds = keras.layers.GlobalAveragePooling1D()(embeds)\n",
        "\n",
        "    # =============================================================================\n",
        "    # ENHANCED MULTI-LAYER CLASSIFICATION HEAD (Experiment 4)\n",
        "    # =============================================================================\n",
        "    # First hidden layer with ReLU activation and dropout\n",
        "    x = keras.layers.Dense(512, activation=\"relu\", name=\"hidden_1\")(embeds)\n",
        "    x = keras.layers.Dropout(0.3, name=\"dropout_1\")(x)\n",
        "    \n",
        "    # Second hidden layer with ReLU activation and dropout\n",
        "    x = keras.layers.Dense(256, activation=\"relu\", name=\"hidden_2\")(x)\n",
        "    x = keras.layers.Dropout(0.2, name=\"dropout_2\")(x)\n",
        "    \n",
        "    # Third hidden layer with ReLU activation and dropout\n",
        "    x = keras.layers.Dense(64, activation=\"relu\", name=\"hidden_3\")(x)\n",
        "    x = keras.layers.Dropout(0.1, name=\"dropout_3\")(x)\n",
        "    \n",
        "    # Final classification layer\n",
        "    # Dense layer with 3 outputs (model A wins, model B wins, tie) and softmax activation\n",
        "    outputs = keras.layers.Dense(3, activation=\"softmax\", name=\"classifier\")(x)\n",
        "\n",
        "    # Create the complete model by connecting inputs to outputs\n",
        "    model = keras.Model(inputs, outputs, name=\"Model_4_Enhanced\")\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compile the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile the model with optimizer, loss function, and evaluation metrics\n",
        "model_3 = create_model_enhanced()\n",
        "# Define the log loss function for the model\n",
        "log_loss_3 = keras.metrics.CategoricalCrossentropy(name=\"log_loss\")\n",
        "# Create a fresh optimizer instance for this model to avoid variable conflicts\n",
        "optimizer_3 = keras.optimizers.Adam(5e-6)\n",
        "# Compile the model with optimizer, loss function, and evaluation metrics\n",
        "model_3.compile(\n",
        "    optimizer=optimizer_3,  # Fresh Adam optimizer with very low learning rate\n",
        "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.02),  # Loss with label smoothing\n",
        "    metrics=[\n",
        "        log_loss_3,  # Custom log loss metric for competition evaluation\n",
        "        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),  # Standard accuracy metric\n",
        "    ],\n",
        "    jit_compile=True # Disable XLA to fix eager execution issues\n",
        ")\n",
        "# Display the model architecture\n",
        "model_3.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if Experiment 3 model training is enabled\n",
        "if CFG.train_experiment_3:\n",
        "    print(\"Training Experiment 3 Model (Model 3 - Enhanced Architecture)...\")\n",
        "    print_memory_usage(\"before training\")\n",
        "\n",
        "    tf.config.run_functions_eagerly(False)\n",
        "    print('Eager execution turned off for all functions.')\n",
        "    \n",
        "    # Create the learning rate callback with current configuration\n",
        "    # Uses batch size and epochs from CFG, and displays the LR schedule plot\n",
        "    lr_cb_3 = get_lr_callback(CFG.batch_size_3, CFG.epochs, plot=True)\n",
        "    # Create the default model checkpoint callback for current experiment\n",
        "    ckpt_cb_3 = create_model_checkpoint_callback(\n",
        "        filepath='model_3.best_weights.weights.h5'\n",
        "    )\n",
        "    # Create the default early stopping callback for current experiment\n",
        "    early_stopping_cb_3 = create_early_stopping_callback()\n",
        "    # Add time logging to the results\n",
        "    time_history_cb_3 = EpochTimeHistory()\n",
        "\n",
        "    # Start training the model\n",
        "    history_3 = model_3.fit(\n",
        "        train_ds,\n",
        "        epochs=CFG.epochs,\n",
        "        validation_data=valid_ds,\n",
        "        verbose = 1,\n",
        "        callbacks=[\n",
        "            lr_cb_3,\n",
        "            ckpt_cb_3,\n",
        "            early_stopping_cb_3,\n",
        "            time_history_cb_3\n",
        "        ]\n",
        "    )\n",
        "else:\n",
        "    print(\"Experiment 3 Model training is DISABLED. Skipping training...\")\n",
        "    history_3 = None\n",
        "\n",
        "print_memory_usage(\"after training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process Results of Model (only if training was enabled):\n",
        "if CFG.train_experiment_3 and history_3 is not None:\n",
        "    results_1 = evaluate_model(\n",
        "        model=model_3,\n",
        "        valid_ds=valid_ds, \n",
        "        history=history_3,\n",
        "        model_name=\"Experiment 3 Model (Model 3 - Enhanced Architecture)\",\n",
        "        show_plots=True\n",
        "    )\n",
        "else:\n",
        "    print(\"Experiment 3 Model evaluation skipped - training was disabled.\")\n",
        "    results_1 = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qsNOvyUoeyj"
      },
      "source": [
        "# 9.&nbsp;Final Prediction and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the best model\n",
        "best_model = model_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka8gqTlWKIl_"
      },
      "source": [
        "## Create Predicitons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract text data from the processed test dataframe\n",
        "# Convert the 'options' column (containing processed text pairs) to a list\n",
        "test_texts = test_psp1_df.options.tolist()\n",
        "\n",
        "# Calculate optimal batch size for test dataset\n",
        "# Use the smaller of dataset size or configured batch size to avoid memory issues\n",
        "test_batch_size = min(len(test_psp1_df), CFG.batch_size)\n",
        "\n",
        "# Build the test dataset using the custom build_dataset function\n",
        "# This creates an optimized TensorFlow dataset pipeline for inference\n",
        "test_ds = build_dataset(\n",
        "    test_texts,           # Input text data (prompt + response pairs)\n",
        "    labels=None,          # No labels for test data (inference only)\n",
        "    batch_size=test_batch_size,  # Calculated batch size for efficient processing\n",
        "    shuffle=False         # No shuffling for consistent test results\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions on the test dataset using the trained model\n",
        "# This runs inference on all test samples and returns probability distributions\n",
        "test_preds = best_model.predict(test_ds, verbose=1)\n",
        "print(\"number of predictions:\", len(test_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II9qX_tf2LSL"
      },
      "source": [
        "## Shape Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create submission dataframe with test IDs\n",
        "# Copy only the 'id' column from the test dataframe to maintain sample identification\n",
        "sub_df = test_df[['id']].copy()\n",
        "\n",
        "# Add prediction probabilities as new columns\n",
        "sub_df[CFG.class_names] = test_preds.tolist()\n",
        "\n",
        "# Display the first few rows of the submission dataframe\n",
        "sub_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creates the final submission file for the Kaggle competition\n",
        "sub_df.to_csv('submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Submission Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **TO DO:** ADD Table Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DueXwork2PiL"
      },
      "source": [
        "## Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# {{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CQ4nuAfqRJA"
      },
      "source": [
        "## Explore Errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# {{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSKrlnc92lEz"
      },
      "source": [
        "# 10.&nbsp;Scale the Auto-Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZDLjLP53aIq"
      },
      "source": [
        "## Auto-Classifier Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# {{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad5blW7Os2iX"
      },
      "source": [
        "## Rerun Process for L1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# {{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhOtlzh0jGGe"
      },
      "source": [
        "## Rerun Process for L2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# {{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWv340Xx265l"
      },
      "source": [
        "# 11.&nbsp; Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuHSlWe-O-br"
      },
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNzNW_nH37_9"
      },
      "source": [
        "## Results Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Result Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YlcWEw_8Bp-"
      },
      "source": [
        "\n",
        "**Baseline Results**\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "**Hyperparameter Tuning Results**\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "**Best Model Results**\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "**Best Model Performance**\n",
        "\n",
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEQfc7PI38Tq"
      },
      "source": [
        "## Model Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql8QLBc2LoEy"
      },
      "source": [
        "### Model Comparisons and Findings\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "#### Baseline Results\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "#### Hyperparameter Tuning\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "#### Best Model Results\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "#### Performance Breakdown (Best Model)\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "#### Conclusion\n",
        "\n",
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlXB6itm4D4r"
      },
      "source": [
        "## Concluding Observations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGbGMDoNOr-w"
      },
      "source": [
        "## Patterns and Conclusions Across the Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8ofvfgBOsLZ"
      },
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb2X9yUX4FOn"
      },
      "source": [
        "# 12.&nbsp; References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq8tcmLa4FXR"
      },
      "source": [
        "**Kaggle Competition**\n",
        "\n",
        "- [1] Wei-lin Chiang, Lianmin Zheng, Lisa Dunlap, Joseph E. Gonzalez, Ion Stoica, Paul Mooney, Sohier Dane, Addison Howard, and Nate Keating. LLM Classification Finetuning. https://kaggle.com/competitions/llm-classification-finetuning, 2024. Kaggle.\n",
        "\n",
        "**Documentation and References**\n",
        "\n",
        "- [2] Addison Howard. LMSYS: KerasNLP Starter. https://www.kaggle.com/code/addisonhoward/lmsys-kerasnlp-starter, 2024. Kaggle.\n",
        "- [3] tt195361. LMSYS: Keras NLP Starter with some changes. https://www.kaggle.com/code/tt195361/lmsys-keras-nlp-starter-with-some-changes#Data-Analysis, 2025. Kaggle.\n",
        "- [4] Adel Anseur. LLM Classification finetuning DeBERTA. https://www.kaggle.com/code/adelanseur/llm-classification-finetuning-deberta  2025. Kaggle.\n",
        "\n",
        "**Prior Work Items Referenced**\n",
        "\n",
        "- [5] Thomas Bohn. deep-learing-gan-monet-painting.ipynb. 2025. https://github.com/TOM-BOHN/MsDS-deep-learing-gan-monet-painting/tree/main\n",
        "- [6] Thomas Bohn. deep-learing-rnn-disaster-tweets.ipynb 2025. https://github.com/TOM-BOHN/MsDS-deep-learing-rnn-disaster-tweets\n",
        "\n",
        "**AI Tools Leveraged**\n",
        "\n",
        "- Cursor.AI was used to aggressively document getting started code that was undocumented in the tutorial.\n",
        "- Cursor.AI was used to support the formatting of markdown tables and text blocks.\n",
        "- Cursor.AI was used to write git commit messages when writing to the repo and tracking checkpoints.\n",
        "- Gemini AI was used to analyze and understand referenced code from other projects and repositories.\n",
        "- Gemini AI was used to debug and resolve errors when running notebooks.\n",
        "- Grammarly was used for spelling and grammar correction during the writing process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMWZ5mItuBlVRpXzeiXWthd",
      "collapsed_sections": [
        "r9augt6L_Wnb",
        "6BtBwi2SKW2N"
      ],
      "mount_file_id": "1h_D7cL2gGhflQZVfZeEActICb25iEJfa",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
